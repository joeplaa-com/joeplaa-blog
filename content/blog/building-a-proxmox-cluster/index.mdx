---
title: Building a Proxmox cluster
date: 2021-10-01
tags: []
excerpt: "This is the first post in a line of (planned) Proxmox how-to posts. In this first post I'll configure a server and install Proxmox and do some basic configuration."
cover: 'cover.png'
author: 'Joep van de Laarschot'
published: true
---

## Introduction

For years I wanted to build my own proper server setup, but all kinds of "rational" reasons prevented me from doing so. Having your own server is too expensive, too wasteful with electricity, too much work, too much time, too much risk. Well, I still agree that all of them are true, but fuck it, I wanted my own servers.

After years of struggling with a -more than decent- custom build home server, I decided to buy some proper equipment. My idea for what I wanted exactly was still really vague. And because I still consider this a semi-hobby project ([jodiBooks](https://jodibooks.com) is my main project), I looked for a cheap used server. After some browsing through classifieds on the Dutch site [Tweakers](https://tweakers.net/servers/aanbod/), I found two ads for two servers from the same seller and decided to buy them both and see what would happen.

That was 3 months ago and, man, I learned a lot in this short period. This has been a hard and frustrating, but ultimately satifying ride. And I am glad I finally have some time to write everything down (ðŸ¤ž*please don't crash now and have me start from scratch*ðŸ¤ž).

This is the first post in a line of (planned) [Proxmox](https://www.proxmox.com) how-to posts. In these posts I want to compile everything I found on multiple websites, blogs and forums in hopes that this guide will prevent you, at least some, frustrations. In this first post I'll configure a server and install [Proxmox](https://www.proxmox.com) and do some basic configuration. That should be a good starting point for running the first VM's.

### What do we want to do

This is my personal list of things this server (or servers) should do:

* Store lots of data
* Run virtual machines
* Do network routing
* Automate lots of stuff for jodiBooks
* Let me fiddle with [crypto/web3](https://www.freecodecamp.org/news/what-is-web3/)

### Hardware selection

Depending on your situation, you can either start with a plan (see previous paragraph) and look for hardware that fits or you already have hardware and than start planning. For this guide it doesn't really matter what hardware you have around. Although I have a proper server now, I will use my old trusty home server for this guide. If you are building your own server, [these guides](https://www.servethehome.com/buyers-guides/) are a good place to start.

The server in question has a [Supermicro X9SCM](https://www.supermicro.com/products/motherboard/xeon/c202_c204/x9scm-f.cfm) mainboard, [Xeon E3-1230 v2](https://ark.intel.com/content/www/us/en/ark/products/65732/intel-xeon-processor-e3-1230-v2-8m-cache-3-30-ghz.html) CPU and 32 GB of ECC RAM. I also installed an [IBM Serveraid M1115](https://www.amazon.com/IBM-Serveraid-Controller-System-81Y4448/dp/B007V8S0D8) HBA card which has a LSI SAS2008 chip. Back when I build it this was one of the most [recommended HBA chips](https://www.servethehome.com/buyers-guides/top-hardware-components-freenas-nas-servers/top-picks-freenas-hbas/) and can be found cheap on Ebay (search for IBM M1015, IBM M1115, Dell Perc H310).

<Alert type='info'>
    Rebranded cards:

* HP H220 = LSI 9207-8i
* Dell Perc H310 = LSI 9205-8i
* IBM M1015 = LSI 9220-8i
* IBM M1115 = LSI 9211-8i

</Alert>

Another recommendation for a home server is the chassis for which I bought the [Fractal design node 804](https://www.amazon.com/Fractal-Design-Node-Case-Computer/dp/B00JBBH93K). It has lots of space, is very quiet and its fan filters catch a lot of dust.


!['Fractal design node 804 disk space'](fractal-design-disks.jpg)

### Hypervisor selection

The most difficult decision is the choice for a hypervisor. I new ESXi from my first dabbles into home servers, but I ditched it in favor of using Ubuntu with VirtualBox. This is not the way you want to run virtual machines in a production environment, so I tried multiple hypervisors and ultimately settled on using [Proxmox](https://www.proxmox.com) for these reasons:

* Price: free
    * Proxmox is free and open-source as opposed to [ESXi](https://www.vmware.com/products/esxi-and-esx.html) (VMWare)
* Technology: [KVM](https://en.wikipedia.org/wiki/Kernel-based_Virtual_Machine) / [QEMU](https://en.wikipedia.org/wiki/QEMU)
    * Proxmox is based on KVM which is build into the Linux kernel
    * [Big players (AWS) are migrating](https://www.freecodecamp.org/news/aws-just-announced-a-move-from-xen-towards-kvm-so-what-is-kvm/) from [XEN](https://xenproject.org/), the "other" open-source option, to KVM
    * Also, on XEN I couldn't get a VM with [Golem]() to work. Which was to be expected as they wrote: "[We do not support XEN hypervisor](https://handbook.golem.network/troubleshooting/provider-troubleshooting)".
* Ease of use: GUI
    * Proxmox was the only KVM based option that I actually got to a working state.
    * KVM with the [Cockpit](https://cockpit-project.org/) GUI is too limited. Most things still needed to be done through the CLI. I now see that "[The Cockpit Web Console is extendable](https://cockpit-project.org/applications.html)".
    * [OpenNebula](https://opennebula.io/)'s implementation is too complicated. Everything needs to be configured manually.
    * [oVirt](https://ovirt.org/) needs a dedicated storage machine.
    * ESXi so far has the best GUI in my opinion, but the free version is too limited.

## Hardware preparation

After installing the mainboard, CPU, RAM, HBA and hooking it all up, there are two things to really consider/do. What is the harddisk configuration going to be and how to flash the HBA.

### Disks

Normally you would want to install an SSD for the OS. Preferably even two in RAID-1 (mirror) mode. I don't have a free one lying around so for this guide I'll have to excuse myself and use two old harddisks \*shudder\*.

HW Raid vs ZFS

For storage I'm going to use 4 old 2TB disks in RAID-Z1 (RAID-5) mode. There is a lot of discussion going on about the [perceived safety](https://www.digistor.com.au/the-latest/Whether-RAID-5-is-still-safe-in-2019/) of using RAID-5. Search for "[RAID 5 is dead](https://search.brave.com/search?q=raid%205%20is%20dead)" and decide for yourself. I myself lost almost all my data because a RAID-5 array failed, so in my new server I went for RAID-Z2 (RAID-6) with an hot spare).

### Flash HBA

Most HBA cards you can buy are configured with **IR** firmware by default. In this mode the card will take care of the RAID functionality. I don't want that, so I have to (cross)flash the card to **IT** mode. I've done this with a Dell Perc H310 in my Dell server and with the earlier mentioned IBM M1115. The process is basically the same for all cards.

<Alert type='warning'>
I also have an HP server. HP, now HPE, is notorious for being very picky about the hardware you install. If it is not HP labeled/branded, the server doesn't recognize the sensor data and assumes the worst. This results in very high fan speeds and thus an unbearable amount of noise.

So in that server I had to install an HP branded HBA: the HP H220. The one I got was flashed with firmware version 15 (the latest). However, it made my fans "idle" at 30%. [After flashing down to version 13](https://www.reddit.com/r/homelab/comments/evg5in/howto_fix_loud_hp_server_fans_with_an_hp_h220_hba/) the fans are back at 8% idle, which is basically silent.
</Alert>

## Proxmox

<Alert type='info'>
The "official" tutorial on how to install and configure Proxmox can be found here:

<https://forum.proxmox.com/threads/proxmox-beginner-tutorial-how-to-set-up-your-first-virtual-machine-on-a-secondary-hard-disk.59559/>

</Alert>

### Configuration

Proxmox has a lot of configuration options. Obviously many of them are dependent on what you want to achieve. I'll describe some of those in other posts. Here I'll focus on some basic, general configuration settings.

<Alert type='warning'>
    Proxmox config is very delicate. Almost all settings can be configured through the GUI, but sometimes you might have to dig into config files with the CLI. <strong>ALWAYS make a backup before doing so!</strong> I learned the hard way that messing up only one file (especially related to the cluster) can mean a full reinstall of that cluster!
</Alert>

#### Non-subscription repository

#### Hide "no subscription" splash screen

#### Nested hypervisor

#### Add users and groups, security

#### Cluster setup

A lot of quirks, especially if you enable 2FA.

The nodes are linked together in a cluster `JPL-cluster`. This makes it possible to:

* Use a single login
    * Single username-password
    * Single private-public key
* Control the nodes, VM's and containers from a single GUI
* Share configuration (files) between nodes
* Migrate VM's and containers between nodes

